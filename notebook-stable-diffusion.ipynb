{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbfbe89",
   "metadata": {},
   "source": [
    "# 端到端体验AIGC-从SageMaker到Web应用\n",
    "\n",
    "生成式AI技术正在迅速提高，现在可以简单地根据文本输入来生成文本和图像。 本次动手训练营将采用Stable Diffusion🧨模型。\n",
    "\n",
    "Stable Diffusion🧨是一种文本到图像模型，可让您创建逼真的AIGC应用程序。\n",
    "\n",
    "在本次动手训练营中，您将了解到:\n",
    " 1. Stable Diffusion模型生成图片的过程\n",
    " 2. 在Sagemaker Notebook Instance中运行该模型\n",
    " 3. 使用Sagemaker Notebook Instance部署模型并进行推理\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d8464",
   "metadata": {},
   "source": [
    "## 1.准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55254b1a",
   "metadata": {},
   "source": [
    "### 1.1 安装及环境配置工作 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536af417",
   "metadata": {},
   "source": [
    "#### 检查环境版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66083a9",
   "metadata": {},
   "source": [
    "#### 安装Notebook运行模型所需的库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum-config-manager --disable docker-ce-stable\n",
    "!sudo yum -y install pigz\n",
    "!pip install -U pip\n",
    "!pip install -U transformers==4.29.2 diffusers==0.16.1 ftfy accelerate\n",
    "!pip install -U torch==1.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install -U sagemaker\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa562670",
   "metadata": {},
   "source": [
    "### 1.2 下载模型文件\n",
    "\n",
    "#### 目前Stable Diffusion发布了Stable Diffusion V1和Stable Diffusion V2版本。\n",
    "\n",
    "起初的[Stable Diffusion V1](https://github.com/CompVis/stable-diffusion) 由 [CompVis](https://ommer-lab.com) 领导，改变了开源人工智能模型的性质，并在全球范围内催生了数百个其他模型和创新。它是所有软件中最快达到 10K Github star 的软件之一，在不到两个月的时间内飙升至 33K star。\n",
    "\n",
    "Stable Diffusion V1使用*降采样因子8*(downsampling-factor 8)自动编码器和860M UNet和 CLIP ViT-L/14文本编码器用于扩散模型。该模型在 256x256 图像上进行预训练，然后在 512x512 图像上进行微调。\n",
    "\n",
    "与最初的 V1 版本相比，[Stable Diffusion V2](https://github.com/Stability-AI/StableDiffusion) 提供了许多重大改进和特性，具体表现在：\n",
    "\n",
    "Stable Diffusion V2版本包含一个具有鲁棒性的文本生成图像模型，在全新的文本编码器 (OpenCLIP) 上训练而成，与早期的 V1 版本相比，文本 -> 图像模型大大提高了图像生成质量，可以生成默认分辨率为 512x512 像素和 768x768 像素的图像。\n",
    "\n",
    "- [Stable Diffusion Launch Announcement](https://stability.ai/blog/stable-diffusion-announcement)\n",
    "- [Stable Diffusion 2.0 Release](https://stability.ai/blog/stable-diffusion-v2-release)\n",
    "\n",
    "#### Stable Diffusion的开源模型文件存放在Hugging Face上。\n",
    "\n",
    "Hugging Face🤗 是一个 AI/ML 社区和平台，早期靠 Transformers 模型库和高质量社区受到关注。用户可以在 Hugging Face 上托管和共享 ML 模型、数据集，也可以构建、训练和部署模型。截至目前，Hugging Face 上共有 7.7 万个预训练模型，以 NLP 模型为主，目前 NLP 模型占比为 50%，2022 年初为 70%，这一比例未来会继续下降。Hugging Face 现在是 NLP 领域的 GitHub，未来希望成为整个 ML 领域的 GitHub，并逐渐向 ML Workflow 的其他环节渗透。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda39128",
   "metadata": {},
   "source": [
    "### 安装git lfs以克隆模型仓库 （git lfs支持大文件上传与下载）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0351bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo sudo amazon-linux-extras install epel -y\n",
    "!sudo yum-config-manager --enable epel\n",
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77e9b2",
   "metadata": {},
   "source": [
    "### 设定模型版本的环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8853697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Stable Diffusion model from HuggingFace\n",
    "\n",
    "#### Stable Diffusion V1\n",
    "SD_SPACE=\"runwayml/\"\n",
    "SD_MODEL = \"stable-diffusion-v1-5\"\n",
    "SD_EXCLUDE_MODEL=\"!v1-5-pruned.ckpt\"\n",
    "\n",
    "#### Stable Diffusion V2\n",
    "# SD_SPACE=\"stabilityai/\"\n",
    "# SD_MODEL = \"stable-diffusion-2-1\"\n",
    "# SD_EXCLUDE_MODEL=\"!v2-1_768-nonema-pruned.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3932cf",
   "metadata": {},
   "source": [
    "### 克隆模型仓库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated time to spend v1(3min),v2(4min)\n",
    "%cd ~/SageMaker\n",
    "!echo $(date)\n",
    "!printf \"=======Current Path========%s\\n\"\n",
    "!rm -rf $SD_MODEL\n",
    "!mkdir $SD_MODEL\n",
    "%cd $SD_MODEL\n",
    "!git init\n",
    "#Include / Exclude file\n",
    "!git config core.sparseCheckout true\n",
    "!echo \"/*\" >> .git/info/sparse-checkout\n",
    "!echo \"!**/*.safetensors\" >> .git/info/sparse-checkout\n",
    "!echo $SD_EXCLUDE_MODEL >> .git/info/sparse-checkout\n",
    "#Checkout and pull file\n",
    "!git remote add -f master https://huggingface.co/$SD_SPACE$SD_MODEL\n",
    "!git pull master main\n",
    "!rm -rf .git\n",
    "%cd ~/SageMaker\n",
    "!printf \"=======Folder========%s\\n$(ls)\\n\"\n",
    "!echo $(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe3b23",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.在Notebook中配置并使用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd041ff9",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "from diffusers import StableDiffusionPipeline\n",
    "# Load stable diffusion\n",
    "pipe = StableDiffusionPipeline.from_pretrained(SD_MODEL, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84a5dc",
   "metadata": {},
   "source": [
    "### 使用GPU进行运算并设定参数\n",
    "\n",
    "为模型设定输入参数，可使用的部分参数如下：\n",
    "- prompt (`str` or `List[str]`):\n",
    "  - 引导图像生成的文本提示或文本列表\n",
    "- height (`int`, *optional*, 默认为 V1模型可支持到512像素，V2模型可支持到768像素):\n",
    "  - 生成图像的高度（以像素为单位）\n",
    "- width (`int`, *optional*,  默认为 V1模型可支持到512像素，V2模型可支持到768像素):\n",
    "  - 生成图像的宽度（以像素为单位）\n",
    "- num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "  - 降噪步数。更多的去噪步骤通常会以较慢的推理为代价获得更高质量的图像\n",
    "- guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "  - 较高的指导比例会导致图像与提示密切相关，但会牺牲图像质量。 如果指定，它必须是一个浮点数。 guidance_scale<=1 被忽略。\n",
    "- negative_prompt (`str` or `List[str]`, *optional*):\n",
    "  - 不引导图像生成的文本或文本列表。不使用时忽略，必须与prompt类型一致（如果 guidance_scale 小于 1 则忽略）\n",
    "- num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "  - 每个提示生成的图像数量\n",
    "  \n",
    "更多参数请参考：https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\n",
    "\n",
    "\n",
    "\n",
    "*GPU内存不够怎么办？*\n",
    "- *试一试分辨率小一点的图片*\n",
    "- *减少生成图片的数量*\n",
    "- *升级机型*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1874593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move Model to the GPU\n",
    "torch.cuda.empty_cache()\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# V1 Max-H:512,Max-W:512\n",
    "# V2 Max-H:768,Max-W:768\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "prompts =[\n",
    "    \"Eiffel tower landing on the Mars\",\n",
    "    \"a photograph of an astronaut riding a horse,van Gogh style\",\n",
    "]\n",
    "generated_images = pipe(\n",
    "    prompt=prompts,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    num_images_per_prompt=1\n",
    ").images  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
    "\n",
    "print(f\"Prompts: {prompts}\\n\")\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "for image in generated_images:\n",
    "    display(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c397444",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.部署模型至Sagemaker Inference Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b7773",
   "metadata": {},
   "source": [
    "构建和训练模型后，您可以将模型部署至终端节点，以中获取预测推理结果。\n",
    "\n",
    "使用 SageMaker 托管服务部署模型有多种选择。 你可以使用 AWS 开发工具包（例如，Python 开发工具包 (Boto3)）、SageMaker Python 开发工具包、AWS CLI 以编程方式部署模型， 或者您可以使用 SageMaker 控制台以交互方式部署模型。\n",
    "\n",
    "使用 SageMaker 托管服务部署模型是一个三步过程，如果您使用 适用于 Python (Boto3)、AWS CLI 或 SageMaker 控制台的 AWS 开发工具包：\n",
    "    \n",
    "    1.在 SageMaker 中创建 SageMaker 模型。\n",
    "    2.为 HTTPS 端点创建端点配置。\n",
    "    3.创建 HTTPS 端点。\n",
    "    \n",
    "使用 SageMaker Python 开发工具包部署模型不需要您创建终端节点配置。 因此，这是一个两步过程：\n",
    "    \n",
    "    1.从创建模型对象 Model可以部署到 HTTPS 端点的类。\n",
    "    2.使用模型对象的预构建创建 HTTPS 端点 deploy()方法。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154598f",
   "metadata": {},
   "source": [
    "### 编写初始化的Sagemaker代码用于部署推理终端节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14eeb63",
   "metadata": {},
   "source": [
    "### 创建自定义推理 inference.py 脚本\n",
    "\n",
    "要使用自定义推理脚本，您需要创建一个 inference.py 脚本。 \n",
    "在我们的示例中，我们将编写 model_fn 以正确加载我们的模型，并编写 predict_fn 以处理数据。\n",
    "\n",
    "更多方法，请参考部署HuggingFace模型到Sagemaker中：https://huggingface.co/docs/sagemaker/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4323cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./$SD_MODEL/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099831cb",
   "metadata": {},
   "source": [
    "#### 为模型创建所需依赖声明的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d35c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./$SD_MODEL/code/requirements.txt\n",
    "diffusers==0.16.1\n",
    "transformers==4.29.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e427ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./$SD_MODEL/code/inference.py\n",
    "import base64\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # Load stable diffusion and move it to the GPU\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "\n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"prompt\", \"\")\n",
    "    # set valid HP for stable diffusion\n",
    "    height = data.pop(\"height\", 512)\n",
    "    width = data.pop(\"width\", 512)\n",
    "    num_inference_steps = data.pop(\"num_inference_steps\", 50)\n",
    "    guidance_scale = data.pop(\"guidance_scale\", 7.5)\n",
    "    num_images_per_prompt = data.pop(\"num_images_per_prompt\", 1)\n",
    "    # run generation with parameters\n",
    "    generated_images = pipe(\n",
    "        prompt=prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "    )[\"images\"]\n",
    "\n",
    "    # create response\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # create response\n",
    "    return {\"generated_images\": encoded_images}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4846889",
   "metadata": {},
   "source": [
    "#### 打包模型并上传至S3桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4509c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package model, Estimated time to spend 5min\n",
    "!echo $(date)\n",
    "\n",
    "!tar --exclude .git --use-compress-program=pigz -pcvf ./$SD_MODEL'.tar.gz' -C ./$SD_MODEL/ .\n",
    "\n",
    "!echo $(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "# upload model.tar.gz to s3, Estimated time to spend 30s(V1), 1min(V2)\n",
    "sd_model_uri=S3Uploader.upload(local_path=f\"{SD_MODEL}.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/stable-diffusion\")\n",
    "print(f\"=======S3 File Location========\\nmodel uploaded to:\\n{sd_model_uri}\")\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7de00",
   "metadata": {},
   "source": [
    "#### 使用HuggingFace将模型部署至SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init variables\n",
    "huggingface_model = {}\n",
    "predictor = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99931527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model[SD_MODEL] = HuggingFaceModel(\n",
    "    model_data=sd_model_uri, # path to your model and script\n",
    "    role=role, # iam role with permissions to create an Endpoint\n",
    "    transformers_version=\"4.26.0\", # transformers version used\n",
    "    pytorch_version=\"1.13.1\", # pytorch version used\n",
    "    py_version='py39', # python version used\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the endpoint endpoint, Estimated time to spend 5min(V1), 8min(V2)\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "predictor[SD_MODEL] = huggingface_model[SD_MODEL].deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=f\"{SD_MODEL}-endpoint\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{datetime.datetime.now()}\")\n",
    "print(f\"\\n{SD_MODEL}-endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f4442",
   "metadata": {},
   "source": [
    "#### 基于推理终端节点生成自定义图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804494ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "    base64_image = base64.b64decode(image_string)\n",
    "    buffer = BytesIO(base64_image)\n",
    "    return Image.open(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run prediction\n",
    "response = predictor[SD_MODEL].predict(data={\n",
    "    \"prompt\": [\n",
    "        \"Eiffel tower landing on the Mars\",\n",
    "#         \"a photograph of an astronaut riding a horse\",\n",
    "    ],\n",
    "    \"height\" : 512,\n",
    "    \"width\" : 512,\n",
    "    \"num_images_per_prompt\":1\n",
    "  }\n",
    ")\n",
    "\n",
    "#decode images\n",
    "decoded_images = [decode_base64_image(image) for image in response[\"generated_images\"]]\n",
    "\n",
    "#visualize generation\n",
    "for image in decoded_images:\n",
    "    display(image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
